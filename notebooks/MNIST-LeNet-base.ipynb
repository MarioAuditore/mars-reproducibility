{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the second experiment with LeNet on MNIST dataset from our paper \"MARS: Masked Automatic Ranks Selection in Tensor Decompositions\".\n",
    "\n",
    "Compression mode: **base** (none)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mars import MARSLoss, get_MARS_attr, set_MARS_attr\n",
    "from models import LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 228  # set random seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and hyperparameters definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression modes parameters\n",
    "modes_dict = {\n",
    "    \"base\": dict(pi=np.nan, alpha=np.nan),\n",
    "    \"compress\": dict(pi=1e-2, alpha=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters section\n",
    "# In this experiment, we take enough epochs to guarantee complete convergence.\n",
    "# One can take fewer but adjust the temperature annealing schedule appropriately.\n",
    "n_epochs = 100  \n",
    "batch_size = 128\n",
    "lr = 5e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.97\n",
    "temp_anneal = lambda t: max(1e-2, gamma * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/sergej/data\"\n",
    "test_batch = 2048\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "trainset = datasets.MNIST(root=data_dir,\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "testset = datasets.MNIST(root=data_dir,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset,\n",
    "                                         batch_size=test_batch,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_idx, mode=\"soft\", save=True, load=True):\n",
    "    \"\"\"\n",
    "    Train the model or load the trained one.\n",
    "    \n",
    "    Parameters are:\n",
    "        model_idx : int\n",
    "            Model index to load or save.\n",
    "        mode : str in {'soft', 'hard'}\n",
    "            Compression mode.\n",
    "        save : bool\n",
    "            Whether to save the trained model.\n",
    "        load : bool\n",
    "            Whether to load the trained model.\n",
    "    \"\"\"\n",
    "    model_directory_path = f\"../models/MNIST-LeNet/{mode}/\"\n",
    "    prefix = str(model_idx)\n",
    "        \n",
    "    model_path = model_directory_path + prefix + '-model.pt'\n",
    "    losses_path = model_directory_path + prefix + '-losses.npy'\n",
    "    print(\"Model path: \", model_path)\n",
    "\n",
    "    if save and not os.path.exists(model_directory_path):\n",
    "        os.makedirs(model_directory_path)\n",
    "        \n",
    "    model = LeNet(config, **modes_dict[mode]).to(device)\n",
    "    \n",
    "    if load and os.path.isfile(model_path):\n",
    "        # load trained model parameters from disk\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        losses = np.load(losses_path)\n",
    "        print('Loaded model parameters from disk.')\n",
    "        return model, losses\n",
    "    \n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    criterion = MARSLoss(model, len(trainset), cross_entropy)\n",
    "    optimizer = optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=weight_decay)\n",
    "#     scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, **{\n",
    "        \"steps_per_epoch\": len(trainloader),\n",
    "        \"epochs\": n_epochs,\n",
    "        \"anneal_strategy\": \"cos\",\n",
    "        \"max_lr\": lr,\n",
    "        \"pct_start\": 0.1\n",
    "    })\n",
    "\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    losses = []\n",
    "    log_step = len(trainloader)\n",
    "    best_train_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        losses.append(0.0)\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses[-1] += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # update statistics\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "                predicted = outputs.argmax(-1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = correct / total\n",
    "        test_acc = eval_model(model)\n",
    "        temp = get_MARS_attr(model, \"temperature\")\n",
    "        print('[%d] \\t Loss: %.3f \\t Train Acc: %.2f%% \\t Test Acc: %.2f%% \\t T: %.3f' %\n",
    "              (epoch + 1, \n",
    "               running_loss / log_step,\n",
    "               100 * train_acc,\n",
    "               100 * test_acc, \n",
    "               np.nan if temp is None else temp))\n",
    "                \n",
    "        losses[-1] /= i + 1\n",
    "        \n",
    "        if save:\n",
    "            if train_acc > best_train_acc:\n",
    "                torch.save(model.state_dict(), model_path[:-3] + \"-best_train.pt\")\n",
    "                best_train_acc = train_acc\n",
    "                best_train_epoch = epoch + 1\n",
    "            if test_acc > best_test_acc:\n",
    "                torch.save(model.state_dict(), model_path[:-3] + \"-best_test.pt\")\n",
    "                best_test_acc = test_acc\n",
    "                best_test_epoch = epoch + 1\n",
    "        \n",
    "        temp = get_MARS_attr(model, \"temperature\")\n",
    "        if temp is not None:\n",
    "            new_temp = temp_anneal(temp)\n",
    "            set_MARS_attr(model, \"temperature\", new_temp)\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('Finished Training.')\n",
    "    print(\"Best train accuracy:\\t%.2f%% on epoch %d\" % (100 * best_train_acc, best_train_epoch))\n",
    "    print(\"Best test accuracy:\\t%.2f%% on epoch %d\" % (100 * best_test_acc, best_test_epoch))\n",
    "    \n",
    "    if save:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        np.save(losses_path, losses)\n",
    "        print('Saved model parameters to disk.')\n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    \"Evaluate a single model on test set.\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = outputs.argmax(-1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ensemble(models):\n",
    "    \"Evaluate the whole ensemble on test set.\"\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = torch.stack([model(images) for model in models])\n",
    "            outputs = torch.softmax(outputs, -1)\n",
    "            outputs = outputs.mean(0)\n",
    "                \n",
    "            predicted = outputs.argmax(-1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_info(model):\n",
    "    \"Plot model masks probabilities, print compression info and return total compression.\"\n",
    "    MARS_layers = [l for l in model.modules() if isinstance(l, MARS)]\n",
    "    \n",
    "    ranks_list = []\n",
    "    totals, dofs = [], []\n",
    "\n",
    "    for l in MARS_layers:\n",
    "        print(\"Layer: \", l.tensorized_model)\n",
    "        phi_logits_list = l.phi_logits_list\n",
    "        F = l.F\n",
    "        eval_logits_threshold = l.eval_logits_threshold\n",
    "        p_threshold = F(torch.tensor(eval_logits_threshold)).item()\n",
    "        ranks = []\n",
    "\n",
    "        for m, logits in enumerate(phi_logits_list, 1):\n",
    "            logits = logits.detach().cpu()\n",
    "            probs = F(logits).data.numpy()\n",
    "            \n",
    "            plt.title(f\"Mask {m}\")\n",
    "            plt.bar(np.arange(1, len(probs) + 1), probs)\n",
    "            plt.xlabel('Rank')\n",
    "            plt.ylabel(r'$\\phi$ value')\n",
    "            plt.hlines(p_threshold, 0, len(probs) + 1, linestyles='--')\n",
    "            plt.text(0, p_threshold * 1.05, 'Rounding threshold')\n",
    "            plt.show()\n",
    "\n",
    "            rank = (logits > eval_logits_threshold).sum().item()\n",
    "            print(\"#nz ranks: {0}/{1}\".format(rank, len(logits)))\n",
    "            ranks.append(rank)\n",
    "\n",
    "        ranks_list.append(ranks)\n",
    "        print()\n",
    "        \n",
    "        dofs.append(l.tensorized_model.calc_dof(ranks))\n",
    "        totals.append(l.tensorized_model.total)\n",
    "        comp = totals[-1] / dofs[-1]\n",
    "        \n",
    "        print(\"Compression:\\t%.3f\" % comp)\n",
    "        print(100*\"=\")\n",
    "\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    mars_params = sum(logits.numel() for l in MARS_layers for logits in l.phi_logits_list)\n",
    "    mars_params += sum(l.tensorized_model.calc_dof() for l in MARS_layers)\n",
    "    other_params = all_params - mars_params\n",
    "    \n",
    "    total_comp = (other_params + sum(totals)) / (other_params + sum(dofs))\n",
    "    print(\"Total compression:\\t%.3f\" % total_comp)\n",
    "    \n",
    "    return total_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No MARS base training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MNISTLeNetConfig:\n",
    "    mars_enabled = False\n",
    "\n",
    "config = MNISTLeNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:  ../models/CIFAR10-ResNet/base/0-model.pt\n",
      "Training...\n",
      "[1] \t Loss: 1.977 \t Train Acc: 25.71% \t Test Acc: 33.20% \t T: nan\n",
      "[2] \t Loss: 1.698 \t Train Acc: 36.16% \t Test Acc: 42.53% \t T: nan\n",
      "[3] \t Loss: 1.538 \t Train Acc: 42.69% \t Test Acc: 46.38% \t T: nan\n",
      "[4] \t Loss: 1.411 \t Train Acc: 48.17% \t Test Acc: 53.71% \t T: nan\n",
      "[5] \t Loss: 1.278 \t Train Acc: 53.51% \t Test Acc: 55.15% \t T: nan\n",
      "[6] \t Loss: 1.165 \t Train Acc: 57.88% \t Test Acc: 60.92% \t T: nan\n",
      "[7] \t Loss: 1.084 \t Train Acc: 61.06% \t Test Acc: 61.99% \t T: nan\n"
     ]
    }
   ],
   "source": [
    "model, loss = train_model(0, mode=\"base\")\n",
    "acc = eval_model(model)\n",
    "print(f\"Accuracy of base model:\\t%.2f%%\" % (100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:  ../models/CIFAR10-ResNet/base/0-model.pt\n",
      "Training...\n",
      "[1] \t Loss: 1.868 \t Train Acc: 29.70% \t Test Acc: 38.66% \t T: nan\n",
      "[2] \t Loss: 1.635 \t Train Acc: 38.73% \t Test Acc: 45.15% \t T: nan\n",
      "[3] \t Loss: 1.446 \t Train Acc: 46.39% \t Test Acc: 52.74% \t T: nan\n",
      "[4] \t Loss: 1.305 \t Train Acc: 52.62% \t Test Acc: 56.86% \t T: nan\n",
      "[5] \t Loss: 1.180 \t Train Acc: 57.26% \t Test Acc: 58.43% \t T: nan\n",
      "[6] \t Loss: 1.078 \t Train Acc: 61.42% \t Test Acc: 65.48% \t T: nan\n",
      "[7] \t Loss: 0.994 \t Train Acc: 64.53% \t Test Acc: 65.82% \t T: nan\n",
      "[8] \t Loss: 0.910 \t Train Acc: 67.63% \t Test Acc: 71.90% \t T: nan\n",
      "[9] \t Loss: 0.848 \t Train Acc: 69.86% \t Test Acc: 72.57% \t T: nan\n",
      "[10] \t Loss: 0.774 \t Train Acc: 72.59% \t Test Acc: 73.15% \t T: nan\n",
      "[11] \t Loss: 0.734 \t Train Acc: 74.08% \t Test Acc: 76.56% \t T: nan\n",
      "[12] \t Loss: 0.681 \t Train Acc: 76.15% \t Test Acc: 75.33% \t T: nan\n",
      "[13] \t Loss: 0.644 \t Train Acc: 77.58% \t Test Acc: 79.91% \t T: nan\n",
      "[14] \t Loss: 0.596 \t Train Acc: 79.35% \t Test Acc: 79.39% \t T: nan\n",
      "[15] \t Loss: 0.568 \t Train Acc: 80.28% \t Test Acc: 80.20% \t T: nan\n",
      "[16] \t Loss: 0.543 \t Train Acc: 80.98% \t Test Acc: 81.64% \t T: nan\n",
      "[17] \t Loss: 0.518 \t Train Acc: 81.77% \t Test Acc: 82.89% \t T: nan\n",
      "[18] \t Loss: 0.491 \t Train Acc: 82.91% \t Test Acc: 82.12% \t T: nan\n",
      "[19] \t Loss: 0.473 \t Train Acc: 83.53% \t Test Acc: 82.12% \t T: nan\n",
      "[20] \t Loss: 0.454 \t Train Acc: 84.30% \t Test Acc: 83.43% \t T: nan\n",
      "[21] \t Loss: 0.436 \t Train Acc: 84.89% \t Test Acc: 83.69% \t T: nan\n",
      "[22] \t Loss: 0.424 \t Train Acc: 85.26% \t Test Acc: 83.36% \t T: nan\n",
      "[23] \t Loss: 0.409 \t Train Acc: 85.95% \t Test Acc: 82.56% \t T: nan\n",
      "[24] \t Loss: 0.399 \t Train Acc: 86.17% \t Test Acc: 85.45% \t T: nan\n",
      "[25] \t Loss: 0.383 \t Train Acc: 86.56% \t Test Acc: 84.49% \t T: nan\n",
      "[26] \t Loss: 0.372 \t Train Acc: 87.11% \t Test Acc: 85.40% \t T: nan\n",
      "[27] \t Loss: 0.363 \t Train Acc: 87.44% \t Test Acc: 82.97% \t T: nan\n",
      "[28] \t Loss: 0.350 \t Train Acc: 87.81% \t Test Acc: 84.71% \t T: nan\n",
      "[29] \t Loss: 0.344 \t Train Acc: 87.94% \t Test Acc: 84.41% \t T: nan\n",
      "[30] \t Loss: 0.332 \t Train Acc: 88.42% \t Test Acc: 85.73% \t T: nan\n",
      "[31] \t Loss: 0.325 \t Train Acc: 88.58% \t Test Acc: 85.37% \t T: nan\n",
      "[32] \t Loss: 0.319 \t Train Acc: 88.74% \t Test Acc: 86.29% \t T: nan\n",
      "[33] \t Loss: 0.309 \t Train Acc: 89.11% \t Test Acc: 86.74% \t T: nan\n",
      "[34] \t Loss: 0.300 \t Train Acc: 89.49% \t Test Acc: 87.26% \t T: nan\n",
      "[35] \t Loss: 0.293 \t Train Acc: 89.60% \t Test Acc: 86.31% \t T: nan\n",
      "[36] \t Loss: 0.287 \t Train Acc: 89.94% \t Test Acc: 86.17% \t T: nan\n",
      "[37] \t Loss: 0.285 \t Train Acc: 90.06% \t Test Acc: 86.85% \t T: nan\n",
      "[38] \t Loss: 0.273 \t Train Acc: 90.51% \t Test Acc: 87.18% \t T: nan\n",
      "[39] \t Loss: 0.267 \t Train Acc: 90.56% \t Test Acc: 86.06% \t T: nan\n",
      "[40] \t Loss: 0.265 \t Train Acc: 90.86% \t Test Acc: 86.52% \t T: nan\n",
      "[41] \t Loss: 0.252 \t Train Acc: 91.10% \t Test Acc: 86.54% \t T: nan\n",
      "[42] \t Loss: 0.252 \t Train Acc: 91.03% \t Test Acc: 88.31% \t T: nan\n",
      "[43] \t Loss: 0.244 \t Train Acc: 91.40% \t Test Acc: 87.23% \t T: nan\n",
      "[44] \t Loss: 0.242 \t Train Acc: 91.53% \t Test Acc: 87.52% \t T: nan\n",
      "[45] \t Loss: 0.238 \t Train Acc: 91.66% \t Test Acc: 88.35% \t T: nan\n",
      "[46] \t Loss: 0.237 \t Train Acc: 91.74% \t Test Acc: 87.53% \t T: nan\n",
      "[47] \t Loss: 0.226 \t Train Acc: 92.01% \t Test Acc: 88.77% \t T: nan\n",
      "[48] \t Loss: 0.223 \t Train Acc: 92.18% \t Test Acc: 86.89% \t T: nan\n",
      "[49] \t Loss: 0.218 \t Train Acc: 92.35% \t Test Acc: 88.54% \t T: nan\n",
      "[50] \t Loss: 0.220 \t Train Acc: 92.29% \t Test Acc: 88.33% \t T: nan\n",
      "[51] \t Loss: 0.212 \t Train Acc: 92.56% \t Test Acc: 87.89% \t T: nan\n",
      "[52] \t Loss: 0.203 \t Train Acc: 92.90% \t Test Acc: 88.32% \t T: nan\n",
      "[53] \t Loss: 0.198 \t Train Acc: 93.00% \t Test Acc: 88.69% \t T: nan\n",
      "[54] \t Loss: 0.196 \t Train Acc: 93.06% \t Test Acc: 88.56% \t T: nan\n",
      "[55] \t Loss: 0.194 \t Train Acc: 93.02% \t Test Acc: 89.37% \t T: nan\n",
      "[56] \t Loss: 0.193 \t Train Acc: 93.22% \t Test Acc: 87.86% \t T: nan\n",
      "[57] \t Loss: 0.184 \t Train Acc: 93.55% \t Test Acc: 89.23% \t T: nan\n",
      "[58] \t Loss: 0.181 \t Train Acc: 93.54% \t Test Acc: 88.91% \t T: nan\n",
      "[59] \t Loss: 0.173 \t Train Acc: 93.90% \t Test Acc: 88.14% \t T: nan\n",
      "[60] \t Loss: 0.172 \t Train Acc: 93.87% \t Test Acc: 88.84% \t T: nan\n",
      "[61] \t Loss: 0.169 \t Train Acc: 93.94% \t Test Acc: 88.18% \t T: nan\n",
      "[62] \t Loss: 0.164 \t Train Acc: 94.16% \t Test Acc: 88.99% \t T: nan\n",
      "[63] \t Loss: 0.166 \t Train Acc: 94.19% \t Test Acc: 89.07% \t T: nan\n",
      "[64] \t Loss: 0.159 \t Train Acc: 94.44% \t Test Acc: 89.40% \t T: nan\n",
      "[65] \t Loss: 0.155 \t Train Acc: 94.43% \t Test Acc: 88.18% \t T: nan\n",
      "[66] \t Loss: 0.149 \t Train Acc: 94.75% \t Test Acc: 89.12% \t T: nan\n",
      "[67] \t Loss: 0.145 \t Train Acc: 94.86% \t Test Acc: 89.14% \t T: nan\n",
      "[68] \t Loss: 0.146 \t Train Acc: 94.90% \t Test Acc: 89.71% \t T: nan\n",
      "[69] \t Loss: 0.139 \t Train Acc: 95.10% \t Test Acc: 89.89% \t T: nan\n",
      "[70] \t Loss: 0.136 \t Train Acc: 95.16% \t Test Acc: 89.15% \t T: nan\n",
      "[71] \t Loss: 0.131 \t Train Acc: 95.39% \t Test Acc: 89.77% \t T: nan\n",
      "[72] \t Loss: 0.127 \t Train Acc: 95.50% \t Test Acc: 89.75% \t T: nan\n",
      "[73] \t Loss: 0.122 \t Train Acc: 95.76% \t Test Acc: 89.79% \t T: nan\n",
      "[74] \t Loss: 0.113 \t Train Acc: 95.89% \t Test Acc: 89.49% \t T: nan\n",
      "[75] \t Loss: 0.115 \t Train Acc: 96.05% \t Test Acc: 89.73% \t T: nan\n",
      "[76] \t Loss: 0.111 \t Train Acc: 96.11% \t Test Acc: 89.73% \t T: nan\n",
      "[77] \t Loss: 0.108 \t Train Acc: 96.14% \t Test Acc: 89.06% \t T: nan\n",
      "[78] \t Loss: 0.100 \t Train Acc: 96.52% \t Test Acc: 90.44% \t T: nan\n",
      "[79] \t Loss: 0.095 \t Train Acc: 96.59% \t Test Acc: 89.76% \t T: nan\n",
      "[80] \t Loss: 0.094 \t Train Acc: 96.70% \t Test Acc: 90.31% \t T: nan\n",
      "[81] \t Loss: 0.091 \t Train Acc: 96.92% \t Test Acc: 89.52% \t T: nan\n",
      "[82] \t Loss: 0.086 \t Train Acc: 96.97% \t Test Acc: 90.26% \t T: nan\n",
      "[83] \t Loss: 0.079 \t Train Acc: 97.21% \t Test Acc: 90.35% \t T: nan\n",
      "[84] \t Loss: 0.076 \t Train Acc: 97.38% \t Test Acc: 90.21% \t T: nan\n",
      "[85] \t Loss: 0.077 \t Train Acc: 97.34% \t Test Acc: 90.40% \t T: nan\n",
      "[86] \t Loss: 0.069 \t Train Acc: 97.59% \t Test Acc: 90.47% \t T: nan\n",
      "[87] \t Loss: 0.066 \t Train Acc: 97.67% \t Test Acc: 90.37% \t T: nan\n",
      "[88] \t Loss: 0.063 \t Train Acc: 97.86% \t Test Acc: 90.49% \t T: nan\n",
      "[89] \t Loss: 0.061 \t Train Acc: 97.89% \t Test Acc: 90.57% \t T: nan\n",
      "[90] \t Loss: 0.059 \t Train Acc: 98.07% \t Test Acc: 90.58% \t T: nan\n",
      "[91] \t Loss: 0.055 \t Train Acc: 98.20% \t Test Acc: 90.47% \t T: nan\n",
      "[92] \t Loss: 0.056 \t Train Acc: 98.22% \t Test Acc: 90.68% \t T: nan\n",
      "[93] \t Loss: 0.052 \t Train Acc: 98.33% \t Test Acc: 90.50% \t T: nan\n",
      "[94] \t Loss: 0.049 \t Train Acc: 98.44% \t Test Acc: 90.72% \t T: nan\n",
      "[95] \t Loss: 0.049 \t Train Acc: 98.39% \t Test Acc: 90.77% \t T: nan\n",
      "[96] \t Loss: 0.048 \t Train Acc: 98.42% \t Test Acc: 90.85% \t T: nan\n",
      "[97] \t Loss: 0.046 \t Train Acc: 98.55% \t Test Acc: 90.72% \t T: nan\n",
      "[98] \t Loss: 0.046 \t Train Acc: 98.53% \t Test Acc: 90.74% \t T: nan\n",
      "[99] \t Loss: 0.046 \t Train Acc: 98.50% \t Test Acc: 90.74% \t T: nan\n",
      "[100] \t Loss: 0.046 \t Train Acc: 98.52% \t Test Acc: 90.78% \t T: nan\n",
      "Finished Training.\n",
      "Best train accuracy:\t98.55% on epoch 96\n",
      "Best test accuracy:\t90.85% on epoch 95\n",
      "Saved model parameters to disk.\n",
      "Accuracy of base model:\t90.78%\n"
     ]
    }
   ],
   "source": [
    "model, loss = train_model(0, mode=\"base\")\n",
    "acc = eval_model(model)\n",
    "print(f\"Accuracy of base model:\\t%.2f%%\" % (100 * acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb78158398375d1e4b28932c174230efb2921cdc4e478a712a12696f417f67f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
