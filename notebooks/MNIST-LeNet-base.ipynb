{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the second experiment with LeNet on MNIST dataset from our paper \"MARS: Masked Automatic Ranks Selection in Tensor Decompositions\".\n",
    "\n",
    "Compression mode: **base** (none)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mars import MARSLoss, get_MARS_attr, set_MARS_attr\n",
    "from models import LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 228  # set random seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and hyperparameters definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression modes parameters\n",
    "modes_dict = {\n",
    "    \"base\": dict(pi=np.nan, alpha=np.nan),\n",
    "    \"compress\": dict(pi=1e-2, alpha=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters section\n",
    "# In this experiment, we take enough epochs to guarantee complete convergence.\n",
    "# One can take fewer but adjust the temperature annealing schedule appropriately.\n",
    "n_epochs = 100  \n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "gamma = 0.97\n",
    "temp_anneal = lambda t: max(1e-2, gamma * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sergej/data\"\n",
    "test_batch = 2048\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "trainset = datasets.MNIST(root=data_dir,\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "testset = datasets.MNIST(root=data_dir,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset,\n",
    "                                         batch_size=test_batch,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_idx, mode=\"soft\", save=True, load=True):\n",
    "    \"\"\"\n",
    "    Train the model or load the trained one.\n",
    "    \n",
    "    Parameters are:\n",
    "        model_idx : int\n",
    "            Model index to load or save.\n",
    "        mode : str in {'soft', 'hard'}\n",
    "            Compression mode.\n",
    "        save : bool\n",
    "            Whether to save the trained model.\n",
    "        load : bool\n",
    "            Whether to load the trained model.\n",
    "    \"\"\"\n",
    "    model_directory_path = f\"../models/MNIST-LeNet/{mode}/\"\n",
    "    prefix = str(model_idx)\n",
    "        \n",
    "    model_path = model_directory_path + prefix + '-model.pt'\n",
    "    losses_path = model_directory_path + prefix + '-losses.npy'\n",
    "    print(\"Model path: \", model_path)\n",
    "\n",
    "    if save and not os.path.exists(model_directory_path):\n",
    "        os.makedirs(model_directory_path)\n",
    "        \n",
    "    model = LeNet(config, **modes_dict[mode]).to(device)\n",
    "    \n",
    "    if load and os.path.isfile(model_path):\n",
    "        # load trained model parameters from disk\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        losses = np.load(losses_path)\n",
    "        print('Loaded model parameters from disk.')\n",
    "        return model, losses\n",
    "    \n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    criterion = MARSLoss(model, len(trainset), cross_entropy)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, **{\n",
    "#         \"steps_per_epoch\": len(trainloader),\n",
    "#         \"epochs\": n_epochs,\n",
    "#         \"anneal_strategy\": \"cos\",\n",
    "#         \"max_lr\": lr,\n",
    "#         \"pct_start\": 0.1\n",
    "#     })\n",
    "\n",
    "    print(\"Training...\")\n",
    "    losses = []\n",
    "    log_step = len(trainloader)\n",
    "    best_train_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        losses.append(0.0)\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses[-1] += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             scheduler.step()\n",
    "\n",
    "            # update statistics\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "                predicted = outputs.argmax(-1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = correct / total\n",
    "        test_acc = eval_model(model)\n",
    "        temp = get_MARS_attr(model, \"temperature\")\n",
    "        print('[%d] \\t Loss: %.3f \\t Train Acc: %.2f%% \\t Test Acc: %.2f%% \\t T: %.3f' %\n",
    "              (epoch + 1, \n",
    "               running_loss / log_step,\n",
    "               100 * train_acc,\n",
    "               100 * test_acc, \n",
    "               np.nan if temp is None else temp))\n",
    "                \n",
    "        losses[-1] /= i + 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if save:\n",
    "            if train_acc > best_train_acc:\n",
    "                torch.save(model.state_dict(), model_path[:-3] + \"-best_train.pt\")\n",
    "                best_train_acc = train_acc\n",
    "                best_train_epoch = epoch + 1\n",
    "            if test_acc > best_test_acc:\n",
    "                torch.save(model.state_dict(), model_path[:-3] + \"-best_test.pt\")\n",
    "                best_test_acc = test_acc\n",
    "                best_test_epoch = epoch + 1\n",
    "        \n",
    "        temp = get_MARS_attr(model, \"temperature\")\n",
    "        if temp is not None:\n",
    "            new_temp = temp_anneal(temp)\n",
    "            set_MARS_attr(model, \"temperature\", new_temp)\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('Finished Training.')\n",
    "    print(\"Best train accuracy:\\t%.2f%% on epoch %d\" % (100 * best_train_acc, best_train_epoch))\n",
    "    print(\"Best test accuracy:\\t%.2f%% on epoch %d\" % (100 * best_test_acc, best_test_epoch))\n",
    "    \n",
    "    if save:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        np.save(losses_path, losses)\n",
    "        print('Saved model parameters to disk.')\n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    \"Evaluate a single model on test set.\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = outputs.argmax(-1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ensemble(models):\n",
    "    \"Evaluate the whole ensemble on test set.\"\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = torch.stack([model(images) for model in models])\n",
    "            outputs = torch.softmax(outputs, -1)\n",
    "            outputs = outputs.mean(0)\n",
    "                \n",
    "            predicted = outputs.argmax(-1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_info(model):\n",
    "    \"Plot model masks probabilities, print compression info and return total compression.\"\n",
    "    MARS_layers = [l for l in model.modules() if isinstance(l, MARS)]\n",
    "    \n",
    "    ranks_list = []\n",
    "    totals, dofs = [], []\n",
    "\n",
    "    for l in MARS_layers:\n",
    "        print(\"Layer: \", l.tensorized_model)\n",
    "        phi_logits_list = l.phi_logits_list\n",
    "        F = l.F\n",
    "        eval_logits_threshold = l.eval_logits_threshold\n",
    "        p_threshold = F(torch.tensor(eval_logits_threshold)).item()\n",
    "        ranks = []\n",
    "\n",
    "        for m, logits in enumerate(phi_logits_list, 1):\n",
    "            logits = logits.detach().cpu()\n",
    "            probs = F(logits).data.numpy()\n",
    "            \n",
    "            plt.title(f\"Mask {m}\")\n",
    "            plt.bar(np.arange(1, len(probs) + 1), probs)\n",
    "            plt.xlabel('Rank')\n",
    "            plt.ylabel(r'$\\phi$ value')\n",
    "            plt.hlines(p_threshold, 0, len(probs) + 1, linestyles='--')\n",
    "            plt.text(0, p_threshold * 1.05, 'Rounding threshold')\n",
    "            plt.show()\n",
    "\n",
    "            rank = (logits > eval_logits_threshold).sum().item()\n",
    "            print(\"#nz ranks: {0}/{1}\".format(rank, len(logits)))\n",
    "            ranks.append(rank)\n",
    "\n",
    "        ranks_list.append(ranks)\n",
    "        print()\n",
    "        \n",
    "        dofs.append(l.tensorized_model.calc_dof(ranks))\n",
    "        totals.append(l.tensorized_model.total)\n",
    "        comp = totals[-1] / dofs[-1]\n",
    "        \n",
    "        print(\"Compression:\\t%.3f\" % comp)\n",
    "        print(100*\"=\")\n",
    "\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    mars_params = sum(logits.numel() for l in MARS_layers for logits in l.phi_logits_list)\n",
    "    mars_params += sum(l.tensorized_model.calc_dof() for l in MARS_layers)\n",
    "    other_params = all_params - mars_params\n",
    "    \n",
    "    total_comp = (other_params + sum(totals)) / (other_params + sum(dofs))\n",
    "    print(\"Total compression:\\t%.3f\" % total_comp)\n",
    "    \n",
    "    return total_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No MARS base training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MNISTLeNetConfig:\n",
    "    mars_enabled = False\n",
    "\n",
    "config = MNISTLeNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:  ../models/MNIST-LeNet/base/0-model.pt\n",
      "Training...\n",
      "[1] \t Loss: 1.838 \t Train Acc: 51.69% \t Test Acc: 81.79% \t T: nan\n",
      "[2] \t Loss: 0.502 \t Train Acc: 86.25% \t Test Acc: 90.11% \t T: nan\n",
      "[3] \t Loss: 0.290 \t Train Acc: 91.38% \t Test Acc: 93.73% \t T: nan\n",
      "[4] \t Loss: 0.185 \t Train Acc: 94.44% \t Test Acc: 95.98% \t T: nan\n",
      "[5] \t Loss: 0.121 \t Train Acc: 96.32% \t Test Acc: 97.55% \t T: nan\n",
      "[6] \t Loss: 0.086 \t Train Acc: 97.45% \t Test Acc: 97.95% \t T: nan\n",
      "[7] \t Loss: 0.067 \t Train Acc: 97.95% \t Test Acc: 98.53% \t T: nan\n",
      "[8] \t Loss: 0.058 \t Train Acc: 98.23% \t Test Acc: 98.52% \t T: nan\n",
      "[9] \t Loss: 0.048 \t Train Acc: 98.52% \t Test Acc: 98.26% \t T: nan\n",
      "[10] \t Loss: 0.042 \t Train Acc: 98.68% \t Test Acc: 98.65% \t T: nan\n",
      "[11] \t Loss: 0.037 \t Train Acc: 98.86% \t Test Acc: 98.74% \t T: nan\n",
      "[12] \t Loss: 0.032 \t Train Acc: 98.97% \t Test Acc: 98.71% \t T: nan\n",
      "[13] \t Loss: 0.028 \t Train Acc: 99.14% \t Test Acc: 98.91% \t T: nan\n",
      "[14] \t Loss: 0.024 \t Train Acc: 99.25% \t Test Acc: 98.95% \t T: nan\n",
      "[15] \t Loss: 0.022 \t Train Acc: 99.32% \t Test Acc: 98.99% \t T: nan\n",
      "[16] \t Loss: 0.019 \t Train Acc: 99.39% \t Test Acc: 98.95% \t T: nan\n",
      "[17] \t Loss: 0.016 \t Train Acc: 99.51% \t Test Acc: 99.00% \t T: nan\n",
      "[18] \t Loss: 0.016 \t Train Acc: 99.49% \t Test Acc: 99.10% \t T: nan\n",
      "[19] \t Loss: 0.012 \t Train Acc: 99.60% \t Test Acc: 98.84% \t T: nan\n",
      "[20] \t Loss: 0.012 \t Train Acc: 99.61% \t Test Acc: 98.93% \t T: nan\n",
      "[21] \t Loss: 0.011 \t Train Acc: 99.66% \t Test Acc: 99.06% \t T: nan\n",
      "[22] \t Loss: 0.009 \t Train Acc: 99.69% \t Test Acc: 99.00% \t T: nan\n",
      "[23] \t Loss: 0.008 \t Train Acc: 99.70% \t Test Acc: 99.10% \t T: nan\n",
      "[24] \t Loss: 0.006 \t Train Acc: 99.78% \t Test Acc: 98.85% \t T: nan\n",
      "[25] \t Loss: 0.008 \t Train Acc: 99.70% \t Test Acc: 98.79% \t T: nan\n",
      "[26] \t Loss: 0.006 \t Train Acc: 99.81% \t Test Acc: 99.05% \t T: nan\n",
      "[27] \t Loss: 0.007 \t Train Acc: 99.79% \t Test Acc: 99.04% \t T: nan\n",
      "[28] \t Loss: 0.004 \t Train Acc: 99.87% \t Test Acc: 99.08% \t T: nan\n",
      "[29] \t Loss: 0.006 \t Train Acc: 99.80% \t Test Acc: 98.97% \t T: nan\n",
      "[30] \t Loss: 0.003 \t Train Acc: 99.91% \t Test Acc: 99.17% \t T: nan\n",
      "[31] \t Loss: 0.005 \t Train Acc: 99.82% \t Test Acc: 98.91% \t T: nan\n",
      "[32] \t Loss: 0.004 \t Train Acc: 99.86% \t Test Acc: 98.92% \t T: nan\n",
      "[33] \t Loss: 0.003 \t Train Acc: 99.91% \t Test Acc: 99.22% \t T: nan\n",
      "[34] \t Loss: 0.004 \t Train Acc: 99.84% \t Test Acc: 99.17% \t T: nan\n",
      "[35] \t Loss: 0.003 \t Train Acc: 99.90% \t Test Acc: 99.19% \t T: nan\n",
      "[36] \t Loss: 0.001 \t Train Acc: 99.98% \t Test Acc: 99.13% \t T: nan\n",
      "[37] \t Loss: 0.002 \t Train Acc: 99.93% \t Test Acc: 98.85% \t T: nan\n",
      "[38] \t Loss: 0.005 \t Train Acc: 99.82% \t Test Acc: 98.92% \t T: nan\n",
      "[39] \t Loss: 0.001 \t Train Acc: 99.96% \t Test Acc: 99.02% \t T: nan\n",
      "[40] \t Loss: 0.002 \t Train Acc: 99.94% \t Test Acc: 99.14% \t T: nan\n",
      "[41] \t Loss: 0.003 \t Train Acc: 99.90% \t Test Acc: 98.83% \t T: nan\n",
      "[42] \t Loss: 0.003 \t Train Acc: 99.91% \t Test Acc: 99.01% \t T: nan\n",
      "[43] \t Loss: 0.001 \t Train Acc: 99.97% \t Test Acc: 99.25% \t T: nan\n",
      "[44] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[45] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[46] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.25% \t T: nan\n",
      "[47] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[48] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.25% \t T: nan\n",
      "[49] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.26% \t T: nan\n",
      "[50] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[51] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.25% \t T: nan\n",
      "[52] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[53] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.19% \t T: nan\n",
      "[54] \t Loss: 0.001 \t Train Acc: 99.98% \t Test Acc: 98.77% \t T: nan\n",
      "[55] \t Loss: 0.006 \t Train Acc: 99.81% \t Test Acc: 99.19% \t T: nan\n",
      "[56] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.17% \t T: nan\n",
      "[57] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.30% \t T: nan\n",
      "[58] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.33% \t T: nan\n",
      "[59] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.33% \t T: nan\n",
      "[60] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.29% \t T: nan\n",
      "[61] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.33% \t T: nan\n",
      "[62] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.34% \t T: nan\n",
      "[63] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.32% \t T: nan\n",
      "[64] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.27% \t T: nan\n",
      "[65] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.32% \t T: nan\n",
      "[66] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.31% \t T: nan\n",
      "[67] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.32% \t T: nan\n",
      "[68] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.30% \t T: nan\n",
      "[69] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.31% \t T: nan\n",
      "[70] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.28% \t T: nan\n",
      "[71] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.26% \t T: nan\n",
      "[72] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[73] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.26% \t T: nan\n",
      "[74] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.29% \t T: nan\n",
      "[75] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.23% \t T: nan\n",
      "[76] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[77] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.23% \t T: nan\n",
      "[78] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[79] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.25% \t T: nan\n",
      "[80] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.23% \t T: nan\n",
      "[81] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.25% \t T: nan\n",
      "[82] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.21% \t T: nan\n",
      "[83] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[84] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[85] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.21% \t T: nan\n",
      "[86] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[87] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.23% \t T: nan\n",
      "[88] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[89] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.23% \t T: nan\n",
      "[90] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.20% \t T: nan\n",
      "[91] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[92] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.21% \t T: nan\n",
      "[93] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.21% \t T: nan\n",
      "[94] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[95] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.24% \t T: nan\n",
      "[96] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[97] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[98] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[99] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "[100] \t Loss: 0.000 \t Train Acc: 100.00% \t Test Acc: 99.22% \t T: nan\n",
      "Finished Training.\n",
      "Best train accuracy:\t100.00% on epoch 45\n",
      "Best test accuracy:\t99.34% on epoch 62\n",
      "Saved model parameters to disk.\n",
      "Accuracy of base model:\t99.22%\n",
      "CPU times: user 3min 31s, sys: 1.05 s, total: 3min 32s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model, loss = train_model(0, mode=\"base\")\n",
    "acc = eval_model(model)\n",
    "print(f\"Accuracy of base model:\\t%.2f%%\" % (100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 ms ± 4.43 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def time_model(model, device):\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "%timeit time_model(model, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342 ms ± 5.91 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit time_model(model, \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb78158398375d1e4b28932c174230efb2921cdc4e478a712a12696f417f67f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
