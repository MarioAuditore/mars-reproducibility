{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the second experiment with CIFAR10 on MNIST dataset from our paper \"MARS: Masked Automatic Ranks Selection in Tensor Decompositions\".\n",
    "\n",
    "Compression mode: **base** (none)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mars import MARS, MARSLoss, get_MARS_attr, set_MARS_attr\n",
    "from models import MarsConfig, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 228  # set random seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and hyperparameters definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression modes parameters\n",
    "modes_dict = {\n",
    "    \"base\": dict(pi=np.nan, alpha=np.nan),\n",
    "    \"naive\": dict(pi=1e-2, alpha=2.25),\n",
    "    \"proper\": dict(pi=4e-3, alpha=3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters section\n",
    "# In this experiment, we take enough epochs to guarantee complete convergence.\n",
    "# One can take fewer but adjust the temperature annealing schedule appropriately.\n",
    "n_epochs = 50  \n",
    "batch_size = 128\n",
    "lr = 5e-3\n",
    "weight_decay = 1e-4\n",
    "gamma = 0.94\n",
    "temp_anneal = lambda t: max(1e-2, gamma * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from augmentations import MixupWrapper, CutoutWrapper, CombineWrapper, SmoothOHEWrapper\n",
    "\n",
    "data_dir = \"/home/sergej/data\"\n",
    "test_batch = 2048\n",
    "\n",
    "# p = 0.5\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.RandomApply([\n",
    "#         transforms.ColorJitter(brightness=.5, hue=.1)\n",
    "#     ], p=p),\n",
    "#     transforms.RandomApply([\n",
    "#         transforms.GaussianBlur(kernel_size=(3, 5), sigma=(0.1, 5))\n",
    "#     ], p=p),\n",
    "#     transforms.RandomApply([\n",
    "#         gauss_noise_tensor\n",
    "#     ], p=p),\n",
    "#     transforms.RandomApply([\n",
    "#         transforms.RandomRotation((-15, 15), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "#     ], p=p),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "#     transforms.Resize(32),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root=data_dir, train=True,\n",
    "                            download=True, transform=train_transform)\n",
    "# trainset, validset = torch.utils.data.random_split(trainset, [45000, 5000],\n",
    "#                                                    generator=torch.Generator().manual_seed(42))\n",
    "# trainset = SmoothOHEWrapper(\n",
    "#     CombineWrapper(\n",
    "#         MixupWrapper(trainset, alpha=0.8, p=0.5),\n",
    "#         CutoutWrapper(trainset, size=10, p=0.5)\n",
    "#     ),\n",
    "#     n_classes=10\n",
    "# )\n",
    "trainset = CutoutWrapper(trainset, size=10, p=0.25)\n",
    "testset = datasets.CIFAR10(root=data_dir, train=False,\n",
    "                           download=True, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "# validloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "#                                           shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_idx, mode=\"soft\", save=True, load=True):\n",
    "    \"\"\"\n",
    "    Train the model or load the trained one.\n",
    "    \n",
    "    Parameters are:\n",
    "        model_idx : int\n",
    "            Model index to load or save.\n",
    "        mode : str in {'soft', 'hard'}\n",
    "            Compression mode.\n",
    "        save : bool\n",
    "            Whether to save the trained model.\n",
    "        load : bool\n",
    "            Whether to load the trained model.\n",
    "    \"\"\"\n",
    "    model_directory_path = f\"../models/CIFAR10-ResNet/{mode}/\"\n",
    "    prefix = str(model_idx)\n",
    "        \n",
    "    model_path = model_directory_path + prefix + '-model.pt'\n",
    "    losses_path = model_directory_path + prefix + '-losses.npy'\n",
    "    print(\"Model path: \", model_path)\n",
    "\n",
    "    if save and not os.path.exists(model_directory_path):\n",
    "        os.makedirs(model_directory_path)\n",
    "        \n",
    "    model = ResNet(config, **modes_dict[mode]).to(device)\n",
    "    \n",
    "    if load and os.path.isfile(model_path):\n",
    "        # load trained model parameters from disk\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        losses = np.load(losses_path)\n",
    "        print('Loaded model parameters from disk.')\n",
    "        return model, losses\n",
    "    \n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    criterion = MARSLoss(model, len(trainset), cross_entropy)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr, weight_decay=weight_decay)\n",
    "#     scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, **{\n",
    "        \"steps_per_epoch\": len(trainloader),\n",
    "        \"epochs\": n_epochs,\n",
    "        \"anneal_strategy\": \"cos\",\n",
    "        \"max_lr\": lr,\n",
    "        \"pct_start\": 0.1\n",
    "    })\n",
    "\n",
    "    print(\"Training...\")\n",
    "    losses = []\n",
    "    log_step = len(trainloader)\n",
    "    best_train_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        losses.append(0.0)\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses[-1] += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # update statistics\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "                predicted = outputs.argmax(-1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = correct / total\n",
    "        test_acc = eval_model(model)\n",
    "        temp = get_MARS_attr(model, \"temperature\")\n",
    "        print('[%d] \\t Loss: %.3f \\t Train Acc: %.2f%% \\t Test Acc: %.2f%% \\t T: %.3f' %\n",
    "              (epoch + 1, \n",
    "               running_loss / log_step,\n",
    "               100 * train_acc,\n",
    "               100 * test_acc, \n",
    "               np.nan if temp is None else temp))\n",
    "                \n",
    "        losses[-1] /= i + 1\n",
    "        \n",
    "        if save:\n",
    "            if train_acc > best_train_acc:\n",
    "                torch.save(model.state_dict(), model_path[:-3] + \"-best_train.pt\")\n",
    "                best_train_acc = train_acc\n",
    "                best_train_epoch = epoch + 1\n",
    "            if test_acc > best_test_acc:\n",
    "                torch.save(model.state_dict(), model_path[:-3] + \"-best_test.pt\")\n",
    "                best_test_acc = test_acc\n",
    "                best_test_epoch = epoch + 1\n",
    "        \n",
    "        temp = get_MARS_attr(model, \"temperature\")\n",
    "        if temp is not None:\n",
    "            new_temp = temp_anneal(temp)\n",
    "            set_MARS_attr(model, \"temperature\", new_temp)\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    print('Finished Training.')\n",
    "    print(\"Best train accuracy:\\t%.2f%% on epoch %d\" % (100 * best_train_acc, best_train_epoch))\n",
    "    print(\"Best test accuracy:\\t%.2f%% on epoch %d\" % (100 * best_test_acc, best_test_epoch))\n",
    "    \n",
    "    if save:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        np.save(losses_path, losses)\n",
    "        print('Saved model parameters to disk.')\n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    \"Evaluate a single model on test set.\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = outputs.argmax(-1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ensemble(models):\n",
    "    \"Evaluate the whole ensemble on test set.\"\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = torch.stack([model(images) for model in models])\n",
    "            outputs = torch.softmax(outputs, -1)\n",
    "            outputs = outputs.mean(0)\n",
    "                \n",
    "            predicted = outputs.argmax(-1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_info(model):\n",
    "    \"Plot model masks probabilities, print compression info and return total compression.\"\n",
    "    MARS_layers = [l for l in model.modules() if isinstance(l, MARS)]\n",
    "    \n",
    "    ranks_list = []\n",
    "    totals, dofs = [], []\n",
    "\n",
    "    for l in MARS_layers:\n",
    "        print(\"Layer: \", l.tensorized_model)\n",
    "        phi_logits_list = l.phi_logits_list\n",
    "        F = l.F\n",
    "        eval_logits_threshold = l.eval_logits_threshold\n",
    "        p_threshold = F(torch.tensor(eval_logits_threshold)).item()\n",
    "        ranks = []\n",
    "\n",
    "        for m, logits in enumerate(phi_logits_list, 1):\n",
    "            logits = logits.detach().cpu()\n",
    "            probs = F(logits).data.numpy()\n",
    "            \n",
    "            plt.title(f\"Mask {m}\")\n",
    "            plt.bar(np.arange(1, len(probs) + 1), probs)\n",
    "            plt.xlabel('Rank')\n",
    "            plt.ylabel(r'$\\phi$ value')\n",
    "            plt.hlines(p_threshold, 0, len(probs) + 1, linestyles='--')\n",
    "            plt.text(0, p_threshold * 1.05, 'Rounding threshold')\n",
    "            plt.show()\n",
    "\n",
    "            rank = (logits > eval_logits_threshold).sum().item()\n",
    "            print(\"#nz ranks: {0}/{1}\".format(rank, len(logits)))\n",
    "            ranks.append(rank)\n",
    "\n",
    "        ranks_list.append(ranks)\n",
    "        print()\n",
    "        \n",
    "        dofs.append(l.tensorized_model.calc_dof(ranks))\n",
    "        totals.append(l.tensorized_model.total)\n",
    "        comp = totals[-1] / dofs[-1]\n",
    "        \n",
    "        print(\"Compression:\\t%.3f\" % comp)\n",
    "        print(100*\"=\")\n",
    "\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    mars_params = sum(logits.numel() for l in MARS_layers for logits in l.phi_logits_list)\n",
    "    mars_params += sum(l.tensorized_model.calc_dof() for l in MARS_layers)\n",
    "    other_params = all_params - mars_params\n",
    "    \n",
    "    total_comp = (other_params + sum(totals)) / (other_params + sum(dofs))\n",
    "    print(\"Total compression:\\t%.3f\" % total_comp)\n",
    "    \n",
    "    return total_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No MARS base training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CIFARResNetConfig:\n",
    "    blocks_per_group = (18, 18, 18)\n",
    "    num_classes = 10\n",
    "    width = 16\n",
    "    mars_configs = (\n",
    "        MarsConfig(\n",
    "            enabled=False\n",
    "        ),\n",
    "        MarsConfig(\n",
    "            enabled=False\n",
    "        ),\n",
    "        MarsConfig(\n",
    "            enabled=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "config = CIFARResNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:  ../models/CIFAR10-ResNet/base/0-model.pt\n",
      "Training...\n",
      "[1] \t Loss: 1.867 \t Train Acc: 29.49% \t Test Acc: 39.33% \t T: nan\n",
      "[2] \t Loss: 1.628 \t Train Acc: 39.05% \t Test Acc: 41.56% \t T: nan\n",
      "[3] \t Loss: 1.440 \t Train Acc: 46.72% \t Test Acc: 54.15% \t T: nan\n",
      "[4] \t Loss: 1.304 \t Train Acc: 52.31% \t Test Acc: 55.80% \t T: nan\n",
      "[5] \t Loss: 1.177 \t Train Acc: 57.33% \t Test Acc: 57.28% \t T: nan\n",
      "[6] \t Loss: 1.084 \t Train Acc: 61.17% \t Test Acc: 64.82% \t T: nan\n",
      "[7] \t Loss: 0.995 \t Train Acc: 64.46% \t Test Acc: 66.70% \t T: nan\n",
      "[8] \t Loss: 0.920 \t Train Acc: 67.26% \t Test Acc: 69.22% \t T: nan\n",
      "[9] \t Loss: 0.864 \t Train Acc: 69.34% \t Test Acc: 71.13% \t T: nan\n",
      "[10] \t Loss: 0.784 \t Train Acc: 72.32% \t Test Acc: 72.54% \t T: nan\n",
      "[11] \t Loss: 0.739 \t Train Acc: 73.97% \t Test Acc: 76.40% \t T: nan\n",
      "[12] \t Loss: 0.689 \t Train Acc: 75.78% \t Test Acc: 76.30% \t T: nan\n",
      "[13] \t Loss: 0.647 \t Train Acc: 77.30% \t Test Acc: 77.06% \t T: nan\n",
      "[14] \t Loss: 0.608 \t Train Acc: 78.90% \t Test Acc: 80.53% \t T: nan\n",
      "[15] \t Loss: 0.571 \t Train Acc: 80.31% \t Test Acc: 79.89% \t T: nan\n",
      "[16] \t Loss: 0.549 \t Train Acc: 80.84% \t Test Acc: 80.23% \t T: nan\n",
      "[17] \t Loss: 0.520 \t Train Acc: 81.80% \t Test Acc: 82.74% \t T: nan\n",
      "[18] \t Loss: 0.496 \t Train Acc: 82.70% \t Test Acc: 82.28% \t T: nan\n",
      "[19] \t Loss: 0.471 \t Train Acc: 83.62% \t Test Acc: 82.91% \t T: nan\n",
      "[20] \t Loss: 0.457 \t Train Acc: 84.13% \t Test Acc: 80.66% \t T: nan\n",
      "[21] \t Loss: 0.432 \t Train Acc: 85.00% \t Test Acc: 83.66% \t T: nan\n",
      "[22] \t Loss: 0.413 \t Train Acc: 85.58% \t Test Acc: 83.81% \t T: nan\n",
      "[23] \t Loss: 0.399 \t Train Acc: 86.20% \t Test Acc: 84.13% \t T: nan\n",
      "[24] \t Loss: 0.389 \t Train Acc: 86.49% \t Test Acc: 84.42% \t T: nan\n",
      "[25] \t Loss: 0.372 \t Train Acc: 86.83% \t Test Acc: 85.24% \t T: nan\n",
      "[26] \t Loss: 0.353 \t Train Acc: 87.62% \t Test Acc: 85.65% \t T: nan\n",
      "[27] \t Loss: 0.345 \t Train Acc: 88.03% \t Test Acc: 84.99% \t T: nan\n",
      "[28] \t Loss: 0.335 \t Train Acc: 88.19% \t Test Acc: 85.23% \t T: nan\n",
      "[29] \t Loss: 0.323 \t Train Acc: 88.70% \t Test Acc: 86.26% \t T: nan\n",
      "[30] \t Loss: 0.310 \t Train Acc: 89.25% \t Test Acc: 85.46% \t T: nan\n",
      "[31] \t Loss: 0.303 \t Train Acc: 89.17% \t Test Acc: 84.67% \t T: nan\n",
      "[32] \t Loss: 0.292 \t Train Acc: 89.80% \t Test Acc: 85.96% \t T: nan\n",
      "[33] \t Loss: 0.281 \t Train Acc: 90.17% \t Test Acc: 87.03% \t T: nan\n",
      "[34] \t Loss: 0.269 \t Train Acc: 90.52% \t Test Acc: 87.55% \t T: nan\n",
      "[35] \t Loss: 0.257 \t Train Acc: 90.85% \t Test Acc: 86.38% \t T: nan\n",
      "[36] \t Loss: 0.255 \t Train Acc: 90.94% \t Test Acc: 87.31% \t T: nan\n",
      "[37] \t Loss: 0.244 \t Train Acc: 91.50% \t Test Acc: 87.57% \t T: nan\n",
      "[38] \t Loss: 0.236 \t Train Acc: 91.51% \t Test Acc: 86.64% \t T: nan\n",
      "[39] \t Loss: 0.231 \t Train Acc: 91.77% \t Test Acc: 87.03% \t T: nan\n",
      "[40] \t Loss: 0.220 \t Train Acc: 92.29% \t Test Acc: 87.57% \t T: nan\n",
      "[41] \t Loss: 0.208 \t Train Acc: 92.72% \t Test Acc: 88.06% \t T: nan\n",
      "[42] \t Loss: 0.205 \t Train Acc: 92.71% \t Test Acc: 88.20% \t T: nan\n",
      "[43] \t Loss: 0.200 \t Train Acc: 92.85% \t Test Acc: 88.07% \t T: nan\n",
      "[44] \t Loss: 0.191 \t Train Acc: 93.21% \t Test Acc: 88.22% \t T: nan\n",
      "[45] \t Loss: 0.187 \t Train Acc: 93.39% \t Test Acc: 87.82% \t T: nan\n",
      "[46] \t Loss: 0.183 \t Train Acc: 93.47% \t Test Acc: 87.64% \t T: nan\n",
      "[47] \t Loss: 0.172 \t Train Acc: 93.95% \t Test Acc: 87.49% \t T: nan\n",
      "[48] \t Loss: 0.169 \t Train Acc: 93.99% \t Test Acc: 87.70% \t T: nan\n",
      "[49] \t Loss: 0.170 \t Train Acc: 94.12% \t Test Acc: 87.76% \t T: nan\n",
      "[50] \t Loss: 0.158 \t Train Acc: 94.37% \t Test Acc: 87.36% \t T: nan\n",
      "[51] \t Loss: 0.157 \t Train Acc: 94.52% \t Test Acc: 88.37% \t T: nan\n",
      "[52] \t Loss: 0.147 \t Train Acc: 94.77% \t Test Acc: 88.59% \t T: nan\n",
      "[53] \t Loss: 0.144 \t Train Acc: 94.83% \t Test Acc: 88.72% \t T: nan\n",
      "[54] \t Loss: 0.140 \t Train Acc: 95.01% \t Test Acc: 87.70% \t T: nan\n",
      "[55] \t Loss: 0.136 \t Train Acc: 95.18% \t Test Acc: 88.70% \t T: nan\n",
      "[56] \t Loss: 0.132 \t Train Acc: 95.33% \t Test Acc: 88.35% \t T: nan\n",
      "[57] \t Loss: 0.123 \t Train Acc: 95.61% \t Test Acc: 89.22% \t T: nan\n",
      "[58] \t Loss: 0.119 \t Train Acc: 95.84% \t Test Acc: 88.55% \t T: nan\n",
      "[59] \t Loss: 0.112 \t Train Acc: 95.96% \t Test Acc: 88.82% \t T: nan\n",
      "[60] \t Loss: 0.112 \t Train Acc: 96.16% \t Test Acc: 89.57% \t T: nan\n",
      "[61] \t Loss: 0.105 \t Train Acc: 96.42% \t Test Acc: 89.02% \t T: nan\n",
      "[62] \t Loss: 0.099 \t Train Acc: 96.62% \t Test Acc: 89.48% \t T: nan\n",
      "[63] \t Loss: 0.098 \t Train Acc: 96.57% \t Test Acc: 89.42% \t T: nan\n",
      "[64] \t Loss: 0.094 \t Train Acc: 96.72% \t Test Acc: 89.37% \t T: nan\n",
      "[65] \t Loss: 0.093 \t Train Acc: 96.75% \t Test Acc: 88.61% \t T: nan\n",
      "[66] \t Loss: 0.082 \t Train Acc: 97.22% \t Test Acc: 89.60% \t T: nan\n",
      "[67] \t Loss: 0.079 \t Train Acc: 97.20% \t Test Acc: 89.46% \t T: nan\n",
      "[68] \t Loss: 0.076 \t Train Acc: 97.38% \t Test Acc: 89.56% \t T: nan\n",
      "[69] \t Loss: 0.077 \t Train Acc: 97.30% \t Test Acc: 89.77% \t T: nan\n",
      "[70] \t Loss: 0.073 \t Train Acc: 97.45% \t Test Acc: 89.75% \t T: nan\n",
      "[71] \t Loss: 0.067 \t Train Acc: 97.71% \t Test Acc: 89.31% \t T: nan\n",
      "[72] \t Loss: 0.064 \t Train Acc: 97.84% \t Test Acc: 90.11% \t T: nan\n",
      "[73] \t Loss: 0.061 \t Train Acc: 97.92% \t Test Acc: 89.59% \t T: nan\n",
      "[74] \t Loss: 0.057 \t Train Acc: 98.02% \t Test Acc: 89.81% \t T: nan\n",
      "[75] \t Loss: 0.052 \t Train Acc: 98.29% \t Test Acc: 89.90% \t T: nan\n",
      "[76] \t Loss: 0.051 \t Train Acc: 98.25% \t Test Acc: 90.05% \t T: nan\n",
      "[77] \t Loss: 0.050 \t Train Acc: 98.29% \t Test Acc: 89.76% \t T: nan\n",
      "[78] \t Loss: 0.048 \t Train Acc: 98.47% \t Test Acc: 89.88% \t T: nan\n",
      "[79] \t Loss: 0.039 \t Train Acc: 98.72% \t Test Acc: 90.16% \t T: nan\n",
      "[80] \t Loss: 0.041 \t Train Acc: 98.70% \t Test Acc: 90.22% \t T: nan\n",
      "[81] \t Loss: 0.037 \t Train Acc: 98.81% \t Test Acc: 89.77% \t T: nan\n",
      "[82] \t Loss: 0.036 \t Train Acc: 98.84% \t Test Acc: 90.32% \t T: nan\n",
      "[83] \t Loss: 0.032 \t Train Acc: 98.91% \t Test Acc: 90.29% \t T: nan\n",
      "[84] \t Loss: 0.032 \t Train Acc: 98.95% \t Test Acc: 90.18% \t T: nan\n",
      "[85] \t Loss: 0.029 \t Train Acc: 99.11% \t Test Acc: 90.26% \t T: nan\n",
      "[86] \t Loss: 0.028 \t Train Acc: 99.08% \t Test Acc: 90.26% \t T: nan\n",
      "[87] \t Loss: 0.029 \t Train Acc: 99.09% \t Test Acc: 90.40% \t T: nan\n",
      "[88] \t Loss: 0.025 \t Train Acc: 99.20% \t Test Acc: 90.27% \t T: nan\n",
      "[89] \t Loss: 0.026 \t Train Acc: 99.23% \t Test Acc: 90.57% \t T: nan\n",
      "[90] \t Loss: 0.026 \t Train Acc: 99.24% \t Test Acc: 90.67% \t T: nan\n",
      "[91] \t Loss: 0.022 \t Train Acc: 99.34% \t Test Acc: 90.70% \t T: nan\n",
      "[92] \t Loss: 0.025 \t Train Acc: 99.29% \t Test Acc: 90.58% \t T: nan\n",
      "[93] \t Loss: 0.023 \t Train Acc: 99.27% \t Test Acc: 90.70% \t T: nan\n",
      "[94] \t Loss: 0.023 \t Train Acc: 99.33% \t Test Acc: 90.77% \t T: nan\n",
      "[95] \t Loss: 0.020 \t Train Acc: 99.34% \t Test Acc: 90.78% \t T: nan\n",
      "[96] \t Loss: 0.022 \t Train Acc: 99.34% \t Test Acc: 90.73% \t T: nan\n",
      "[97] \t Loss: 0.021 \t Train Acc: 99.39% \t Test Acc: 90.76% \t T: nan\n",
      "[98] \t Loss: 0.020 \t Train Acc: 99.40% \t Test Acc: 90.86% \t T: nan\n",
      "[99] \t Loss: 0.020 \t Train Acc: 99.38% \t Test Acc: 90.78% \t T: nan\n",
      "[100] \t Loss: 0.021 \t Train Acc: 99.33% \t Test Acc: 90.77% \t T: nan\n",
      "Finished Training.\n",
      "Best train accuracy:\t99.40% on epoch 98\n",
      "Best test accuracy:\t90.86% on epoch 98\n",
      "Saved model parameters to disk.\n",
      "Accuracy of base model:\t90.77%\n"
     ]
    }
   ],
   "source": [
    "model, loss = train_model(0, mode=\"base\")\n",
    "acc = eval_model(model)\n",
    "print(f\"Accuracy of base model:\\t%.2f%%\" % (100 * acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb78158398375d1e4b28932c174230efb2921cdc4e478a712a12696f417f67f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
